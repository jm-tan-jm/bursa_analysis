{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba82369-4344-4f3b-bd43-28f604b7e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tabula-py library to convert pdf to Excel - use keyword and get the line to the end of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d3c9e5-d18b-4ac3-a79d-884f8c733158",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAILED to write the content to an existing Excel file,\n",
    "## so this version will try to store all the tables in a df variable and convert one time only to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10b836a-be33-4254-b707-4d985942b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST RUN !!!\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = r\"path\\to\\Java\\jdk-21\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e4a4-d0ef-4f1c-8a65-bf6cf3dd15a5",
   "metadata": {},
   "source": [
    "#### About parameters below\n",
    "- 'keyword_pattern' - ways to set Regex:\n",
    "    1. contain keyword - re.compile(re.escape(\"NET INTEREST INCOME\"),re.IGNORECASE)\n",
    "    2. contain keyword only - re.compile(r'^\\s*\\bNET INTEREST INCOME\\b\\s*$', re.IGNORECASE)\n",
    "- 'check_nan' - if to check other columns are NaN - useful to check if the keyword is heading, example: (keyword_pattern=abc)\n",
    "|column1|column2|column3|result|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|123|NaN|NaN|no|\n",
    "|abc|NaN|NaN|yes|\n",
    "|abc|NaN|123|no|\n",
    "|NaN|abc|NaN|yes|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a55fcc0-cc48-4dc6-8991-27fdbcc033f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 1 - search with keyword and return page number, line number, and column number\n",
    "import tabula\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_keyword_in_table(pdf_path, keyword_pattern, check_nan=True):\n",
    "    # Define the area of the full page in points\n",
    "    area = [0, 0, 1000, 1000]  # [top, left, bottom, right]\n",
    "    \n",
    "    table = tabula.read_pdf(input_path=pdf_path, pages='all', stream=True, encoding='utf-8', area=area)\n",
    "\n",
    "    keyword_locations = []\n",
    "    # tabula.read_pdf will return a list of tables, so \"page_num\" is the index for each table and df is each table\n",
    "    for page_num, df in enumerate(table, start=1):\n",
    "        # df.itertuples() - Iterate over DataFrame rows as namedtuples\n",
    "        # \n",
    "        for row_index, row in enumerate(df.itertuples(), start=1):\n",
    "            keyword_found = False\n",
    "            for col_index, cell in enumerate(row[1:], start=1):  # Use enumerate to get col_index\n",
    "                if re.search(keyword_pattern, str(cell)):\n",
    "                    if check_nan and any(pd.notnull(row[j]) for j in range(1, len(row)) if j != col_index):\n",
    "                        # If check_nan is True, check if any other column in the same row is not NaN\n",
    "                        break  # Break out of inner loop and skip this row\n",
    "                    keyword_locations.append((str(page_num), str(row_index), str(col_index)))\n",
    "                    keyword_found = True\n",
    "                    break  # Break out of inner loop if keyword is found in a column\n",
    "            if keyword_found:\n",
    "                break  # Break out of outer loop if keyword is found\n",
    "    return keyword_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b24677d-269c-4f1b-8ff2-76ee276850d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 2 - remove duplicated pages (if locations have same page, get the combination of lowest line number)\n",
    "\n",
    "def unique_page(locations):\n",
    "    # Dictionary to store the smallest second number for each unique first number\n",
    "    smallest_second_numbers = {}\n",
    "    \n",
    "    # Iterate through the list of lists\n",
    "    for sublist in locations:\n",
    "        if not sublist:  # Check if sublist is empty\n",
    "            continue  # Skip empty sublists\n",
    "        number = sublist[0][0]\n",
    "        second_number = sublist[0][1]\n",
    "    \n",
    "        if number in smallest_second_numbers:\n",
    "            if second_number < smallest_second_numbers[number]:\n",
    "                smallest_second_numbers[number] = second_number\n",
    "        else:\n",
    "            smallest_second_numbers[number] = second_number\n",
    "    \n",
    "    # Filter the list of lists based on the smallest second number for each unique first number\n",
    "    # wrong example - filtered_data = [[(number, second, third)] for [number, second, third] in locations if second == smallest_second_numbers[number]]\n",
    "    filtered_data = [[(number, second, third)] for sublist in locations for (number, second, third) in sublist if second == smallest_second_numbers.get(number)]\n",
    "    \n",
    "    # Sort the filtered data based on the first number\n",
    "    filtered_data_sorted = sorted(filtered_data, key=lambda x: int(x[0][0]))\n",
    "\n",
    "    return filtered_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a586894e-922f-4fd8-93e7-bd259ce12755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 3 - get the table and store to a df (from the keyword line to the end of the page) \n",
    "## 'location' parameter - get from the 1st function\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def keyword_content(pdf_path, location, check_nan=True):\n",
    "\n",
    "    for page, line, column in location:\n",
    "        # Define the area of the full page in points\n",
    "        area = [0, 0, 1000, 1000]  # [top, left, bottom, right]\n",
    "\n",
    "        # Read the PDF and extract tables\n",
    "        tables = tabula.read_pdf(input_path=pdf_path, pages=page, stream=True, encoding='utf-8', area=area)\n",
    "\n",
    "        # extract only from the line until the end\n",
    "        df = tables[0].loc[int(line)-1:]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af8eddb-c11a-4549-8267-7ff33068ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 4 - convert the data frame to Excel\n",
    "def df_to_excel(dfs, output_path):\n",
    "    try:\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            # Add the DataFrame to a new sheet in the Excel file\n",
    "            for i, table in enumerate(dfs):\n",
    "                table.to_excel(writer, sheet_name=f\"Table_{i+1}\", index=False, header=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b0e3bd-334a-41f7-a042-29b6800ebab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 5 - combined all the functions above\n",
    "def pdf_to_excel_by_keyword(heading_keywords, keywords, pdf_path=None, output_path=None, check_nan=True):\n",
    "    if pdf_path is None or output_path is None:\n",
    "        raise ValueError(\"Please provide data for 'pdf_path' or 'output_path'\")\n",
    "\n",
    "    if not heading_keywords and not keywords:\n",
    "        raise ValueError(\"Heading keywords and Keywords list are empty\")\n",
    "    \n",
    "    # to store all the pattern for heading_keyword and keyword\n",
    "    all_patterns = []\n",
    "\n",
    "    # lines that contains only keyword, dont have anything infront and after\n",
    "    for heading_keyword in heading_keywords:\n",
    "        heading_keyword_pattern = re.compile(rf'^\\s*\\b{heading_keyword}\\b\\s*$', re.IGNORECASE)\n",
    "        all_patterns.append(heading_keyword_pattern)\n",
    "\n",
    "    # lines that contains keyword anyway\n",
    "    for keyword in keywords:\n",
    "        keyword_pattern=re.compile(re.escape(keyword), re.IGNORECASE)\n",
    "        all_patterns.append(keyword_pattern)\n",
    "    \n",
    "    locations = []\n",
    "    for pattern in all_patterns:\n",
    "        # function 1 - get the location of a keyword\n",
    "        location = find_keyword_in_table(pdf_path=pdf_path, keyword_pattern=pattern, check_nan=check_nan)\n",
    "        locations.append(location)\n",
    "    \n",
    "    # function 2 - remove duplicated page from the locations - remain the one with lowest line number\n",
    "    locations_unique_page = unique_page(locations=locations)\n",
    "    \n",
    "    dfs = []\n",
    "    for location in locations_unique_page:\n",
    "        # function 3 - get the content of the keyword (from the keyword's line to the end of the page)\n",
    "        df = keyword_content(pdf_path=pdf_path, location=location, check_nan=check_nan)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # function 4 - convert the df above to Excel - each tabel to each sheet\n",
    "    df_to_excel(dfs=dfs, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9d5d1a-1ed5-4764-a986-c2c0c6744c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines that contains only keyword, dont have anything infront and after\n",
    "heading_keywords = [\"INCOME STATEMENTS\",\n",
    "                    \"NET INTEREST INCOME\",\n",
    "                    \"KEY INTEREST BEARING ASSETS AND LIABILITIES\",\n",
    "                    \"STATEMENTS OF FINANCIAL POSITION\"]\n",
    "\n",
    "# lines that contains keyword anyway\n",
    "keywords = [\"Impaired loans, advances and financing by economic purpose\",\n",
    "            \"Impaired loans, advances and financing by geographical distribution\",\n",
    "            \"Loans, advances and financing analysed by type of customers\",\n",
    "            \"Loans, advances and financing analysed by geographical locations\",\n",
    "            \"Loans, advances and financing analysed by economic purpose\",\n",
    "            \"Movements in impaired loans, advances and financing (“impaired loans”)\",\n",
    "            \"The capital adequacy ratios of the Group and of the Bank\",\n",
    "            \"The breakdown of RWA by each major risk categories for the Group and the Bank\"]\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = \"path\\to\\pdfs\"\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "for pdf_file in pdf_files:\n",
    "    year = pdf_file.split('AR')[-1].split('_')[0]\n",
    "    pdf_path = os.path.join(folder_path, pdf_file)\n",
    "    output_path = os.path.join(folder_path, f\"Maybank-AR{year}-raw.xlsx\")\n",
    "\n",
    "    pdf_to_excel_by_keyword(heading_keywords=heading_keywords, keywords=keywords, check_nan=True, pdf_path=pdf_path, output_path=output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
